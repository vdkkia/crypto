version: "3.8"

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:6.1.0
    container_name: zookeeper
    ports:
      - "${ZOOKEEPER_EXPOSED_PORT}:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-log:/var/lib/zookeeper/log
  broker:
    image: confluentinc/cp-kafka:6.1.0
    container_name: broker
    ports:
      - "19092:19092"
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,CONNECTIONS_FROM_HOST://localhost:19092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONNECTIONS_FROM_HOST:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_LOG_RETENTION_MINUTES: 30
      KAFKA_LOG_RETENTION_BYTES: 10733418240
    volumes:
      - broker-data:/var/lib/kafka/data
  schemareg:
    image: confluentinc/cp-schema-registry:6.1.0
    container_name: schemareg
    restart: unless-stopped
    ports:
      - "${SCHEMA_REGISTRY_EXPOSED_PORT}:8081"
    depends_on:
      - zookeeper
      - broker
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper:2181
      SCHEMA_REGISTRY_HOST_NAME: schemareg
  kafkaconnect:
    image: confluentinc/cp-kafka-connect:6.1.0
    container_name: kafkaconnect
    ports:
      - "${KAFKA_CONNECT_EXPOSED_PORT}:8083"
    depends_on:
      - broker
      - zookeeper
      - schemareg
    environment:
      # networking
      CONNECT_BOOTSTRAP_SERVERS: "broker:9092"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafkaconnect"
      CONNECT_REST_PORT: 8083
      # kafka
      CONNECT_GROUP_ID: "kc"
      CONNECT_CONFIG_STORAGE_TOPIC: "kc-config"
      CONNECT_OFFSET_STORAGE_TOPIC: "kc-offset"
      CONNECT_STATUS_STORAGE_TOPIC: "kc-status"
      # convertors
      CONNECT_KEY_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_VALUE_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: "http://schemareg:8081"
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: "http://schemareg:8081"
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
    volumes:
      - kafkaconnect-data:/tmp/data
  kafdrop:
    image: obsidiandynamics/kafdrop:3.27.0
    container_name: kafdrop
    restart: unless-stopped
    ports:
      - "${KAFDROP_PORT}:9000"
    environment:
      KAFKA_BROKERCONNECT: "broker:9092"
      JVM_OPTS: "-Xms16M -Xmx48M -Xss180K -XX:-TieredCompilation -XX:+UseStringDeduplication -noverify"
    depends_on:
      - broker
  mongodb:
    image: mongo:4.4.4
    container_name: coinmongo
    restart: unless-stopped
    command: mongod --auth --config /etc/mongod.conf
    environment:
      - MONGO_INITDB_ROOT_USERNAME
      - MONGO_INITDB_ROOT_PASSWORD
      - MONGO_INITDB_USERNAME
      - MONGO_INITDB_PASSWORD
      - MONGO_INITDB_DATABASE
    volumes:
      - ./mongodb/001_init-mongo.sh:/docker-entrypoint-initdb.d/001_init-mongo.sh
      - ./mongodb/mongod.conf:/etc/mongod.conf
      - mongo-data:/data/db
    ports:
      - "${MONGO_EXPOSED_PORT}:27017"
  redis:
    image: redis:6.2.1
    restart: unless-stopped
    container_name: coinredis
    command: redis-server --appendonly yes
    ports:
      - "${REDIS_EXPOSED_PORT}:6379"
    volumes:
      - redis-data:/data
      # - ./redis/redis.conf:/usr/local/etc/redis/redis.conf
  db:
    image: timescale/timescaledb:2.1.0-pg13
    restart: unless-stopped
    container_name: coindb
    ports:
      - "${POSTGRES_EXPOSED_PORT}:5432"
    environment:
      - POSTGRES_DB
      - POSTGRES_USER
      - POSTGRES_PASSWORD
    volumes:
      - db-data:/var/lib/postgresql/data
      - ./db/init:/docker-entrypoint-initdb.d/
  backoffice:
    container_name: coinbackoffice
    restart: unless-stopped
    build:
      context: backoffice
      dockerfile: Dockerfile
    ports:
      - "${BACKOFFICE_EXPOSED_PORT}:3000"
    environment:
      - NODE_ENV
      - POSTGRES_URI=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}
      - KAFKA_BROKER_HOST=broker:9092
      - KAFKA_CLIENT_ID=backoffice_admin
    depends_on:
      - db
      - broker
    command: bash -c "sleep 30 && cd /usr/src/app && npm install && npm run start"
    volumes:
      - ./backoffice:/usr/src/app
      - /usr/src/app/node_modules
  scraper:
    container_name: scraper
    restart: unless-stopped
    build:
      context: scraper
      dockerfile: Dockerfile
    environment:
      - NODE_ENV
      - REDIS_URI=redis://redis:6379
      - MONGO_URI=mongodb://${MONGO_INITDB_USERNAME}:${MONGO_INITDB_PASSWORD}@mongodb:27017/${MONGO_INITDB_DATABASE}
      - PROXY_URI
      - ACTIVE_SECONDS_PER_MIN
      - COOKIE_STOCK_SIZE
      - AXIOS_TIMEOUT
      - MAX_CONCURRENCY
      - SLACK_INCOMING_WEBHOOK
      - KAFKA_BROKER_HOST=broker:9092
      - KAFKA_CLIENT_ID=nodejs_1
    depends_on:
      - redis
      - backoffice
      - mongodb
    command: bash -c "sleep 40 && cd /usr/src/app && npm install && npm run start"
    volumes:
      - ./scraper:/usr/src/app
      - /usr/src/app/node_modules
      - scraper_logs:/logs
  normalizer:
    container_name: normalizer
    restart: unless-stopped
    build:
      context: normalizer
      dockerfile: Dockerfile
    depends_on:
      - backoffice
    environment:
      - NODE_ENV
      - PYTHONUNBUFFERED=1
      - KAFKA_BROKER_HOST=broker:9092
    entrypoint:
      - bash
      - -c
      - |
        echo 'Giving Kafka a bit of time to start upâ€¦'
        sleep 40
        if [ $NODE_ENV = 'production' ]; then python -u index.py; else watchmedo auto-restart --recursive --pattern="*.py" --directory="." python index.py; fi;
    volumes:
      - ./normalizer:/usr/src/app

volumes:
  zookeeper-data:
  zookeeper-log:
  broker-data:
  kafkaconnect-data:
  db-data:
  redis-data:
  mongo-data:
  scraper_logs:
