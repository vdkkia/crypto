version: "3.8"

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:6.1.0
    container_name: zookeeper
    ports:
      - "${ZOOKEEPER_EXPOSED_PORT}:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-log:/var/lib/zookeeper/log
    logging:
      driver: "json-file"
      options:
        max-size: "500m"
        max-file: "10"
  broker:
    image: confluentinc/cp-kafka:6.1.0
    container_name: broker
    ports:
      - "19092:19092"
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONNECTIONS_FROM_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,CONNECTIONS_FROM_HOST://localhost:19092
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_LOG_RETENTION_MINUTES: 30
      KAFKA_LOG_RETENTION_BYTES: 10733418240
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    volumes:
      - broker-data:/var/lib/kafka/data
    logging:
      driver: "json-file"
      options:
        max-size: "500m"
        max-file: "10"
  schemareg:
    image: confluentinc/cp-schema-registry:6.1.0
    container_name: schemareg
    restart: unless-stopped
    ports:
      - "8081:8081"
    depends_on:
      - broker
      - zookeeper
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper:2181
      SCHEMA_REGISTRY_HOST_NAME: schemareg
    logging:
      driver: "json-file"
      options:
        max-size: "500m"
        max-file: "10"
  connect:
    image: confluentinc/cp-kafka-connect:6.1.0
    container_name: connect
    build:
      context: connect
      dockerfile: Dockerfile
    ports:
      - "8083:8083"
    depends_on:
      - broker
      - schemareg
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "broker:9092"
      CONNECT_REST_ADVERTISED_HOST_NAME: "connect"
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: connect
      CONNECT_CONFIG_STORAGE_TOPIC: _kafka-connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _kafka-connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _kafka-connect-status
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: "http://schemareg:8081"
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: "http://schemareg:8081"
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_LOG4J_ROOT_LOGLEVEL: "INFO"
      CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components/,/connectors/"
    volumes:
      - connect-data:/tmp/data
    logging:
      driver: "json-file"
      options:
        max-size: "500m"
        max-file: "10"
  ksqldb:
    image: confluentinc/ksqldb-server:0.15.0
    hostname: ksqldb
    restart: unless-stopped
    container_name: ksqldb
    depends_on:
      - broker
      - connect
      - schemareg
    ports:
      - "8088:8088"
    environment:
      KSQL_LISTENERS: http://0.0.0.0:8088
      KSQL_BOOTSTRAP_SERVERS: broker:9092
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: "true"
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: "true"
      KSQL_KSQL_CONNECT_URL: http://connect:8083
      KSQL_KSQL_SCHEMA_REGISTRY_URL: http://schemareg:8081
      KSQL_KSQL_SERVICE_ID: confluent_fkh_01
      KSQL_KSQL_HIDDEN_TOPICS: "^_.*"
      # KSQL_KSQL_QUERIES_FILE: /queries/init.sql
    volumes:
      - ./ksqldb/queries:/queries
    logging:
      driver: "json-file"
      options:
        max-size: "500m"
        max-file: "10"
  kafdrop:
    image: obsidiandynamics/kafdrop:3.27.0
    container_name: kafdrop
    restart: unless-stopped
    ports:
      - "${KAFDROP_PORT}:9000"
    environment:
      KAFKA_BROKERCONNECT: "broker:9092"
      JVM_OPTS: "-Xms16M -Xmx48M -Xss180K -XX:-TieredCompilation -XX:+UseStringDeduplication -noverify"
    depends_on:
      - broker
    logging:
      driver: "json-file"
      options:
        max-size: "500m"
        max-file: "10"
  mongodb:
    image: mongo:4.4.4
    container_name: coinmongo
    restart: unless-stopped
    command: mongod --auth --config /etc/mongod.conf
    environment:
      - MONGO_INITDB_ROOT_USERNAME
      - MONGO_INITDB_ROOT_PASSWORD
      - MONGO_INITDB_USERNAME
      - MONGO_INITDB_PASSWORD
      - MONGO_INITDB_DATABASE
    volumes:
      - ./mongodb/001_init-mongo.sh:/docker-entrypoint-initdb.d/001_init-mongo.sh
      - ./mongodb/mongod.conf:/etc/mongod.conf
      - mongo-data:/data/db
    ports:
      - "${MONGO_EXPOSED_PORT}:27017"
    logging:
      driver: "json-file"
      options:
        max-size: "500m"
        max-file: "10"
  redis:
    image: redis:6.2.1
    restart: unless-stopped
    container_name: coinredis
    command: redis-server --appendonly yes
    ports:
      - "${REDIS_EXPOSED_PORT}:6379"
    volumes:
      - redis-data:/data
      # - ./redis/redis.conf:/usr/local/etc/redis/redis.conf
    logging:
      driver: "json-file"
      options:
        max-size: "500m"
        max-file: "10"
  db:
    image: timescale/timescaledb:2.1.0-pg13
    restart: unless-stopped
    container_name: coindb
    ports:
      - "${POSTGRES_EXPOSED_PORT}:5432"
    environment:
      - POSTGRES_DB
      - POSTGRES_USER
      - POSTGRES_PASSWORD
    volumes:
      - db-data:/var/lib/postgresql/data
      - ./db/init:/docker-entrypoint-initdb.d/
    logging:
      driver: "json-file"
      options:
        max-size: "500m"
        max-file: "10"
  backoffice:
    container_name: coinbackoffice
    restart: unless-stopped
    build:
      context: backoffice
      dockerfile: Dockerfile
    ports:
      - "${BACKOFFICE_EXPOSED_PORT}:3000"
    environment:
      - NODE_ENV
      - POSTGRES_URI=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}
      - KAFKA_BROKER_HOST=broker:9092
      - KAFKA_CLIENT_ID=backoffice_admin
      - KSQLDB_REST_API_BASE=http://ksqldb:8088
      - KAFKA_CONNECT_REST_API_BASE=http://connect:8083
      - KAFKA_SCHEMA_REGISTRY_URI=http://schemareg:8081
      - POSTGRES_HOST=db
      - POSTGRES_PORT=5432
      - POSTGRES_DB
      - POSTGRES_USER
      - POSTGRES_PASSWORD
    depends_on:
      - db
      - broker
    command: bash -c "sleep 60 && cd /usr/src/app && npm install && npm run start"
    volumes:
      - ./backoffice:/usr/src/app
      - /usr/src/app/node_modules
    logging:
      driver: "json-file"
      options:
        max-size: "500m"
        max-file: "10"
  normalizer:
    container_name: normalizer
    restart: unless-stopped
    build:
      context: normalizer
      dockerfile: Dockerfile
    ports:
      - "${NORMALIZER_EXPOSED_PORT}:3000"
    environment:
      - NODE_ENV
      - REDIS_URI=redis://redis:6379
      - KAFKA_BROKER_HOST=broker:9092
      - KAFKA_CLIENT_ID=normalizer
      - KSQLDB_REST_API_BASE=http://ksqldb:8088
    depends_on:
      - redis
      - backoffice
      - broker
    command: bash -c "sleep 90 && cd /usr/src/app && npm install && npm run start"
    volumes:
      - ./normalizer:/usr/src/app
      - /usr/src/app/node_modules
      - normalizer_logs:/logs
    logging:
      driver: "json-file"
      options:
        max-size: "500m"
        max-file: "10"
  scraper:
    container_name: scraper
    restart: unless-stopped
    build:
      context: scraper
      dockerfile: Dockerfile
    environment:
      - NODE_ENV
      - REDIS_URI=redis://redis:6379
      - MONGO_URI=mongodb://${MONGO_INITDB_USERNAME}:${MONGO_INITDB_PASSWORD}@mongodb:27017/${MONGO_INITDB_DATABASE}
      - PROXY_URI
      - ACTIVE_SECONDS_PER_MIN
      - COOKIE_STOCK_SIZE
      - AXIOS_TIMEOUT
      - MAX_CONCURRENCY
      - SLACK_INCOMING_WEBHOOK
      - KAFKA_BROKER_HOST=broker:9092
      - KAFKA_CLIENT_ID=nodejs_1
      - NORMALIZER_API_BASE_URI=http://normalizer:3000
    depends_on:
      - redis
      - backoffice
      - mongodb
      - normalizer
    command: bash -c "sleep 120 && cd /usr/src/app && npm install && npm run start"
    volumes:
      - ./scraper:/usr/src/app
      - /usr/src/app/node_modules
      - scraper_logs:/logs
    logging:
      driver: "json-file"
      options:
        max-size: "500m"
        max-file: "10"

volumes:
  zookeeper-data:
  zookeeper-log:
  broker-data:
  connect-data:
  db-data:
  redis-data:
  mongo-data:
  scraper_logs:
  normalizer_logs:
